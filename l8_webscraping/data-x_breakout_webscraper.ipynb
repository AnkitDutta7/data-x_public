{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://i67.tinypic.com/2jcbwcw.png\" align=\"left\"></img><br><br><br><br>\n",
    "\n",
    "\n",
    "## Breakout Lecture 8: Web scraping & crawling\n",
    "\n",
    "**Author List**: Alexander Fred Ojala\n",
    "\n",
    "**Original Sources**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "**License**: Feel free to do whatever you want to with this code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Run this Notebook in your Python 3 Virtual Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stretch Jupyter coding blocks to fit screen\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bs4 as bs # Beautiful Soup is a Python library for pulling data out of HTML and XML files.\n",
    "import urllib.request # The urllib.request module defines functions and classes which help in opening URLs (mostly HTTP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the Data-X Syllabus site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Always use requests or urllib to open a URL and read in the source code before\n",
    "# using Beautifulsoup to extract the information you want.\n",
    "\n",
    "source = urllib.request.urlopen('https://data-x.blog/syllabus-data-x/').read() # open the URL and read in the content as a string of HTML code\n",
    "print(source) # this is the source code for the site, looks very unstructured right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create your Beautifulsoup object\n",
    "\n",
    "soup = bs.BeautifulSoup(source,features='lxml') # interact with this object, looks like source in browser.\n",
    "\n",
    "# source should be a string of HTML code\n",
    "# features = 'lxml' is the parser. Sometimes it might be better to use 'html.parser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look at the printed source above. Title of the website is <title>Syllabus: Data-X &#8211; Data-X</title>. \n",
    "# Let's try to parse that with beautifulsoup\n",
    "\n",
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# or\n",
    "\n",
    "print(soup.find('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.title.name) # name of the tag for soup.title (should be 'title' of course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# .string on a Tag type object returns a NavigableString type object.\n",
    "\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# On the other hand, .text gets all the child strings and return concatenated using the given separator. \n",
    "# Return type of .text is unicode object.\n",
    "\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example when there is a difference in child strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.tr.string) # returns None, because no string type object in between <tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.tr.text) # returns all child strings within the <td> tags as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.p) #finds first p tag in the HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.find_all('p') #finds all p tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.find_all('p').text # cannot use method on the full ResultSet, place in a loop instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# remove tag\n",
    "# can remove with regex, but here we use beautifulsoups built in functionality\n",
    "for paragraph in soup.find_all('p'):\n",
    "    print(paragraph.text) # returns regular unicode .string returns navigable text\n",
    "    # child tag removes if we have span etc\n",
    "    \n",
    "# as we can see the table is not nicely structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.get_text()) # Get all child strings, concatenated, will print all javascript, wordpress coding text in a nice fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find first url\n",
    "\n",
    "first_url = soup.find('a')\n",
    "print('first_url:', first_url,'\\n')\n",
    "\n",
    "print('Type:',type(first_url))\n",
    "print('Text: ',first_url.text)\n",
    "print('Attributes:',first_url.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for url in soup.find_all('a'):\n",
    "    print(url.get('href')) # get method get specific tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# imagine we only want to extract http links and write them to a file called data-x-urls.txt on a separate line \n",
    "\n",
    "# find all url links at the page\n",
    "links = list()\n",
    "for url in soup.find_all('a'):\n",
    "    link = url.get('href')\n",
    "    if 'http' in link:\n",
    "        print(link)\n",
    "        links.append(link+'\\n')\n",
    "\n",
    "# create/open a txt file with write, will overwrite if there is a file called data-x-urls\n",
    "with open('data-x-urls.txt', 'w') as file: \n",
    "    file.writelines(links) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only find URL's in the navigation bar (tag nav)\n",
    "nav=soup.nav\n",
    "nav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(nav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for url in nav.find_all('a'):\n",
    "    print(url.get('href')) # only links in navigation bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body = soup.body #get content within the <body> tag of the HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print all body text\n",
    "for paragraph in body.find_all('p'):\n",
    "    print(paragraph.text) #might be two body tags. \n",
    "    # Just text from the body\n",
    "    # Scraping for content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all text within div sections, also child tags - or specific div section\n",
    "for div in soup.find_all('div'):\n",
    "    print(div.text) # a lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prints both mobile and html version\n",
    "\n",
    "for div in soup.find_all('div', class_='site-content'):\n",
    "    print(div.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Only get tables, scraping tables and xml documents\n",
    "\n",
    "table = soup.table\n",
    "table = soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table # shows the html code of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_rows = table.find_all('tr') #table.tr or table.find('tr') would only find one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tr in table_rows:\n",
    "    td = tr.find_all('td') # find all table data\n",
    "    row = [i.text for i in td]\n",
    "    print(row) # get all the table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas version of grabbing tables, better\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# requires html5lib: \n",
    "#!conda install --yes html5lib\n",
    "dfs = pd.read_html('https://data-x.blog/syllabus-data-x/',header=0)\n",
    "# header = 0, indicates that first row is header\n",
    "# find all tables and parse them to several data frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(type(dfs))\n",
    "print(len(dfs))\n",
    "df = dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Looks great, but we might want the dates to be the indices and in datetimeformat\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.iloc[:,0] # dates are stored in the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dates = list() # list of better formatted dates\n",
    "for date in df.iloc[:,0]:\n",
    "    d = date.split()[1:] # split date strings and only extract Month plus Day, exclude lecture number\n",
    "    d = '2017 ' + ' '.join(d)\n",
    "    dates.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index=pd.to_datetime(dates) # convert dates to datetime objects and set them as the index\n",
    "df.index.name='Date' #rename the index column to be \"Date\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop('Lec #',axis=1,inplace=True) # Drop the first column, with the old dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1) # to not get ... in the results\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_html('data-x-sched.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth=50 #change back to default max col_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(soup.find_all('img'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.path.basename?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "for link in soup.find_all('img'):\n",
    "    img_url=link.get('src')\n",
    "    \n",
    "    if 'jpg' in img_url: #only check for jpg images\n",
    "        print(img_url)\n",
    "        print(os.path.splitext(os.path.basename(img_url))) # returns final component of pathname and extension as a tuple\n",
    "        filename = os.path.splitext(os.path.basename(img_url))[0] + '.jpg'\n",
    "        urllib.request.urlretrieve(img_url,filename) # urllib requests a file and then writes it to disk\n",
    "    else:\n",
    "        print('EXCLUDED:',img_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XML documents - site maps, all the urls. just between tags\n",
    "# XML human and machine readable.\n",
    "# Newest links: all the links for FIND SITE MAP!\n",
    "# News websites will have sitemaps for politics, bot constantly\n",
    "# tracking news track the sitemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source = urllib.request.urlopen('https://data-x.blog/sitemap.xml').read()\n",
    "soup = bs.BeautifulSoup(source,'xml') # interact with this object, looks like source in brower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.find_all('loc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Bloomberg for news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source = urllib.request.urlopen('https://www.bloomberg.com/feeds/bpol/sitemap_news.xml').read()\n",
    "soup = bs.BeautifulSoup(source,'xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soup.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for news in soup.find_all({'news'}):\n",
    "    print(news.title.text)\n",
    "    print(news.publication_date.text)\n",
    "    print(news.keywords.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example from https://www.ayima.com/guides/how-to-visualize-an-xml-sitemap-using-python.html\n",
    "\n",
    "# Visualize XML sitemap with categories!\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.sportchek.ca/sitemap.xml'\n",
    "url = 'https://www.bloomberg.com/feeds/bpol/sitemap_index.xml'\n",
    "page = requests.get(url)\n",
    "print('Loaded page with: %s' % page)\n",
    "\n",
    "sitemap_index = BeautifulSoup(page.content, 'html.parser')\n",
    "print('Created %s object' % type(sitemap_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = [element.text for element in sitemap_index.findAll('loc')]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_links(url):\n",
    "    ''' Open an XML sitemap and find content wrapped in loc tags. '''\n",
    "\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    links = [element.text for element in soup.findAll('loc')]\n",
    "\n",
    "    return links\n",
    "\n",
    "sitemap_urls = []\n",
    "for url in urls:\n",
    "    links = extract_links(url)\n",
    "    sitemap_urls += links\n",
    "\n",
    "print('Found {:,} URLs in the sitemap'.format(len(sitemap_urls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sitemap_urls.dat', 'w') as f:\n",
    "    for url in sitemap_urls:\n",
    "        f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Categorize a list of URLs by site path.\n",
    "The file containing the URLs should exist in the working directory and be\n",
    "named sitemap_urls.dat. It should contain one URL per line.\n",
    "Categorization depth can be specified by executing a call like this in the\n",
    "terminal (where we set the granularity depth level to 5):\n",
    "    python categorize_urls.py --depth 5\n",
    "The same result can be achieved by setting the categorization_depth variable\n",
    "manually at the head of this file and running the script with:\n",
    "    python categorize_urls.py\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "categorization_depth=3\n",
    "\n",
    "\n",
    "\n",
    "# Main script functions\n",
    "\n",
    "\n",
    "def peel_layers(urls, layers=3):\n",
    "    ''' Builds a dataframe containing all unique page identifiers up\n",
    "    to a specified depth and counts the number of sub-pages for each.\n",
    "    Prints results to a CSV file.\n",
    "    urls : list\n",
    "        List of page URLs.\n",
    "    layers : int\n",
    "        Depth of automated URL search. Large values for this parameter\n",
    "        may cause long runtimes depending on the number of URLs.\n",
    "    '''\n",
    "\n",
    "    # Store results in a dataframe\n",
    "    sitemap_layers = pd.DataFrame()\n",
    "\n",
    "    # Get base levels\n",
    "    bases = pd.Series([url.split('//')[-1].split('/')[0] for url in urls])\n",
    "    sitemap_layers[0] = bases\n",
    "\n",
    "    # Get specified number of layers\n",
    "    for layer in range(1, layers+1):\n",
    "\n",
    "        page_layer = []\n",
    "        for url, base in zip(urls, bases):\n",
    "            try:\n",
    "                page_layer.append(url.split(base)[-1].split('/')[layer])\n",
    "            except:\n",
    "                # There is nothing that deep!\n",
    "                page_layer.append('')\n",
    "\n",
    "        sitemap_layers[layer] = page_layer\n",
    "\n",
    "    # Count and drop duplicate rows + sort\n",
    "    sitemap_layers = sitemap_layers.groupby(list(range(0, layers+1)))[0].count()\\\n",
    "                     .rename('counts').reset_index()\\\n",
    "                     .sort_values('counts', ascending=False)\\\n",
    "                     .sort_values(list(range(0, layers)), ascending=True)\\\n",
    "                     .reset_index(drop=True)\n",
    "\n",
    "    # Convert column names to string types and export\n",
    "    sitemap_layers.columns = [str(col) for col in sitemap_layers.columns]\n",
    "    sitemap_layers.to_csv('sitemap_layers.csv', index=False)\n",
    "\n",
    "    # Return the dataframe\n",
    "    return sitemap_layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sitemap_urls = open('sitemap_urls.dat', 'r').read().splitlines()\n",
    "print('Loaded {:,} URLs'.format(len(sitemap_urls)))\n",
    "\n",
    "print('Categorizing up to a depth of %d' % categorization_depth)\n",
    "sitemap_layers = peel_layers(urls=sitemap_urls,\n",
    "                             layers=categorization_depth)\n",
    "print('Printed {:,} rows of data to sitemap_layers.csv'.format(len(sitemap_layers)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Visualize a list of URLs by site path.\n",
    "This script reads in the sitemap_layers.csv file created by the\n",
    "categorize_urls.py script and builds a graph visualization using Graphviz.\n",
    "Graph depth can be specified by executing a call like this in the\n",
    "terminal:\n",
    "    python visualize_urls.py --depth 4 --limit 10 --title \"My Sitemap\" --style \"dark\" --size \"40\"\n",
    "The same result can be achieved by setting the variables manually at the head\n",
    "of this file and running the script with:\n",
    "    python visualize_urls.py\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "# Set global variables\n",
    "\n",
    "graph_depth = 3  # Number of layers deep to plot categorization\n",
    "limit = 3       # Maximum number of nodes for a branch\n",
    "title = ''       # Graph title\n",
    "style = 'light'  # Graph style, can be \"light\" or \"dark\"\n",
    "size = '8,5'     # Size of rendered PDF graph\n",
    "\n",
    "\n",
    "# Import external library dependencies\n",
    "\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "\n",
    "\n",
    "\n",
    "# Main script functions\n",
    "\n",
    "def make_sitemap_graph(df, layers=3, limit=50, size='8,5'):\n",
    "    ''' Make a sitemap graph up to a specified layer depth.\n",
    "    sitemap_layers : DataFrame\n",
    "        The dataframe created by the peel_layers function\n",
    "        containing sitemap information.\n",
    "    layers : int\n",
    "        Maximum depth to plot.\n",
    "    limit : int\n",
    "        The maximum number node edge connections. Good to set this\n",
    "        low for visualizing deep into site maps.\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Check to make sure we are not trying to plot too many layers\n",
    "    if layers > len(df) - 1:\n",
    "        layers = len(df)-1\n",
    "        print('There are only %d layers available to plot, setting layers=%d'\n",
    "              % (layers, layers))\n",
    "\n",
    "\n",
    "    # Initialize graph\n",
    "    f = graphviz.Digraph('sitemap', filename='sitemap_graph_%d_layer' % layers)\n",
    "    f.body.extend(['rankdir=LR', 'size=\"%s\"' % size])\n",
    "\n",
    "\n",
    "    def add_branch(f, names, vals, limit, connect_to=''):\n",
    "        ''' Adds a set of nodes and edges to nodes on the previous layer. '''\n",
    "\n",
    "        # Get the currently existing node names\n",
    "        node_names = [item.split('\"')[1] for item in f.body if 'label' in item]\n",
    "\n",
    "        # Only add a new branch it it will connect to a previously created node\n",
    "        if connect_to:\n",
    "            if connect_to in node_names:\n",
    "                for name, val in list(zip(names, vals))[:limit]:\n",
    "                    f.node(name='%s-%s' % (connect_to, name), label=name)\n",
    "                    f.edge(connect_to, '%s-%s' % (connect_to, name), label='{:,}'.format(val))\n",
    "\n",
    "\n",
    "    f.attr('node', shape='rectangle') # Plot nodes as rectangles\n",
    "\n",
    "    # Add the first layer of nodes\n",
    "    for name, counts in df.groupby(['0'])['counts'].sum().reset_index()\\\n",
    "                          .sort_values(['counts'], ascending=False).values:\n",
    "        f.node(name=name, label='{} ({:,})'.format(name, counts))\n",
    "\n",
    "    if layers == 0:\n",
    "        return f\n",
    "\n",
    "    f.attr('node', shape='oval') # Plot nodes as ovals\n",
    "    f.graph_attr.update()\n",
    "\n",
    "    # Loop over each layer adding nodes and edges to prior nodes\n",
    "    for i in range(1, layers+1):\n",
    "        cols = [str(i_) for i_ in range(i)]\n",
    "        nodes = df[cols].drop_duplicates().values\n",
    "        for j, k in enumerate(nodes):\n",
    "\n",
    "            # Compute the mask to select correct data\n",
    "            mask = True\n",
    "            for j_, ki in enumerate(k):\n",
    "                mask &= df[str(j_)] == ki\n",
    "\n",
    "            # Select the data then count branch size, sort, and truncate\n",
    "            data = df[mask].groupby([str(i)])['counts'].sum()\\\n",
    "                    .reset_index().sort_values(['counts'], ascending=False)\n",
    "\n",
    "            # Add to the graph\n",
    "            add_branch(f,\n",
    "                       names=data[str(i)].values,\n",
    "                       vals=data['counts'].values,\n",
    "                       limit=limit,\n",
    "                       connect_to='-'.join(['%s']*i) % tuple(k))\n",
    "\n",
    "            print(('Built graph up to node %d / %d in layer %d' % (j, len(nodes), i))\\\n",
    "                    .ljust(50), end='\\r')\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "def apply_style(f, style, title=''):\n",
    "    ''' Apply the style and add a title if desired. More styling options are\n",
    "    documented here: http://www.graphviz.org/doc/info/attrs.html#d:style\n",
    "    f : graphviz.dot.Digraph\n",
    "        The graph object as created by graphviz.\n",
    "    style : str\n",
    "        Available styles: 'light', 'dark'\n",
    "    title : str\n",
    "        Optional title placed at the bottom of the graph.\n",
    "    '''\n",
    "\n",
    "    dark_style = {\n",
    "        'graph': {\n",
    "            'label': title,\n",
    "            'bgcolor': '#3a3a3a',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '18',\n",
    "            'fontcolor': 'white',\n",
    "        },\n",
    "        'nodes': {\n",
    "            'style': 'filled',\n",
    "            'color': 'white',\n",
    "            'fillcolor': 'black',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '14',\n",
    "            'fontcolor': 'white',\n",
    "        },\n",
    "        'edges': {\n",
    "            'color': 'white',\n",
    "            'arrowhead': 'open',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '12',\n",
    "            'fontcolor': 'white',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    light_style = {\n",
    "        'graph': {\n",
    "            'label': title,\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '18',\n",
    "            'fontcolor': 'black',\n",
    "        },\n",
    "        'nodes': {\n",
    "            'style': 'filled',\n",
    "            'color': 'black',\n",
    "            'fillcolor': '#dbdddd',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '14',\n",
    "            'fontcolor': 'black',\n",
    "        },\n",
    "        'edges': {\n",
    "            'color': 'black',\n",
    "            'arrowhead': 'open',\n",
    "            'fontname': 'Helvetica',\n",
    "            'fontsize': '12',\n",
    "            'fontcolor': 'black',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if style == 'light':\n",
    "        apply_style = light_style\n",
    "\n",
    "    elif style == 'dark':\n",
    "        apply_style = dark_style\n",
    "\n",
    "    f.graph_attr = apply_style['graph']\n",
    "    f.node_attr = apply_style['nodes']\n",
    "    f.edge_attr = apply_style['edges']\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read in categorized data\n",
    "sitemap_layers = pd.read_csv('sitemap_layers.csv', dtype=str)\n",
    "# Convert numerical column to integer\n",
    "sitemap_layers.counts = sitemap_layers.counts.apply(int)\n",
    "print('Loaded {:,} rows of categorized data from sitemap_layers.csv'\\\n",
    "        .format(len(sitemap_layers)))\n",
    "\n",
    "print('Building %d layer deep sitemap graph' % graph_depth)\n",
    "f = make_sitemap_graph(sitemap_layers, layers=graph_depth,\n",
    "                       limit=limit, size=size)\n",
    "f = apply_style(f, style=style, title=title)\n",
    "\n",
    "f.render(cleanup=True)\n",
    "print('Exported graph to sitemap_graph_%d_layer.pdf' % graph_depth)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
